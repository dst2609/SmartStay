{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ddf16e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datatable\n",
      "  Obtaining dependency information for datatable from https://files.pythonhosted.org/packages/bb/c5/987fcb116df777d2573c8918b7d7bb391405fa0ca3ed209238bf447b7aac/datatable-1.1.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading datatable-1.1.0-cp311-cp311-win_amd64.whl.metadata (1.8 kB)\n",
      "Downloading datatable-1.1.0-cp311-cp311-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.7 MB 7.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.0/4.7 MB 13.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.1/4.7 MB 16.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.1/4.7 MB 16.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.1/4.7 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.2/4.7 MB 15.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.2/4.7 MB 15.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.7/4.7 MB 13.7 MB/s eta 0:00:00\n",
      "Installing collected packages: datatable\n",
      "Successfully installed datatable-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a877a201-5b56-4e06-99f2-86d526f37b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import csv\n",
    "\n",
    "# def json_to_csv(input_file, output_file):\n",
    "#     with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "#         fieldnames = ['hotel_url', 'author', 'date', 'rating', 'title', 'text', 'property_dict']\n",
    "#         csv_writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "#         csv_writer.writeheader()\n",
    "\n",
    "#         for line in infile:\n",
    "#             data = json.loads(line)\n",
    "#             csv_writer.writerow(data)\n",
    "\n",
    "# # Example usage:\n",
    "# input_file = '../hotelRec/HotelRec.txt'\n",
    "# output_file = 'hotel_reviews.csv'\n",
    "# json_to_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa5a8c9-5f9d-4ec5-9877-7a1b791f13b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50264531\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(df.index)\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcea02cc-6b54-4249-896a-ba0a5230bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           hotel_url        author  \\\n",
      "0  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...  violettaf340   \n",
      "1  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...     Lagaiuzza   \n",
      "2  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...    ashleyn763   \n",
      "3  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...   DavideMauro   \n",
      "4  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...      Alemma11   \n",
      "\n",
      "                  date  rating  \\\n",
      "0  2019-01-01T00:00:00     5.0   \n",
      "1  2016-01-01T00:00:00     5.0   \n",
      "2  2014-10-01T00:00:00     5.0   \n",
      "3  2014-08-01T00:00:00     5.0   \n",
      "4  2013-08-01T00:00:00     4.0   \n",
      "\n",
      "                                              title  \\\n",
      "0                                      Xmas holiday   \n",
      "1                                Baltic, what else?   \n",
      "2                           Excellent in every way!   \n",
      "3                The house of your family's holiday   \n",
      "4  A paradise for children (and parents, of course)   \n",
      "\n",
      "                                                text  \\\n",
      "0  We went here with our kids for Xmas holiday an...   \n",
      "1  We have spent in this hotel our summer holiday...   \n",
      "2  I visited Hotel Baltic with my husband for som...   \n",
      "3  I've travelled quite a numbers of hotels but t...   \n",
      "4  We decided for this family holiday destination...   \n",
      "\n",
      "                                       property_dict  \n",
      "0                                                 {}  \n",
      "1                                                 {}  \n",
      "2    {'service': 5.0, 'location': 5.0, 'value': 5.0}  \n",
      "3  {'service': 5.0, 'cleanliness': 5.0, 'sleep qu...  \n",
      "4  {'sleep quality': 3.0, 'value': 4.0, 'rooms': ...  \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv('../data/hotel_reviews.csv')\n",
    "# Perform computations on the Dask DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d930a0-fff3-4a43-b2f4-5c8b01419159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def create_label_encoders(df: pd.DataFrame) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Create label encoders (mapping dictionaries) for hotel_url and author columns\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing hotel_url and author columns\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], Dict[str, int]]: Hotel URL and author mapping dictionaries\n",
    "    \"\"\"\n",
    "    # Create mapping dictionaries\n",
    "    hotel_mapping = {url: idx for idx, url in enumerate(df['hotel_url'].unique())}\n",
    "    author_mapping = {author: idx for idx, author in enumerate(df['author'].unique())}\n",
    "    \n",
    "    return hotel_mapping, author_mapping\n",
    "\n",
    "def encode_hotel_author(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode hotel_url and author columns to integer values\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing hotel_url and author columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with encoded columns added\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Create mapping dictionaries\n",
    "    hotel_mapping, author_mapping = create_label_encoders(df)\n",
    "    \n",
    "    # Add encoded columns\n",
    "    df_encoded['hotel_id'] = df_encoded['hotel_url'].map(hotel_mapping)\n",
    "    df_encoded['author_id'] = df_encoded['author'].map(author_mapping)\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Number of unique hotels: {len(hotel_mapping)}\")\n",
    "    print(f\"Number of unique authors: {len(author_mapping)}\")\n",
    "    \n",
    "    return df_encoded, hotel_mapping, author_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf8c33c-509a-4fab-a477-0da0546d1de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique hotels: 365057\n",
      "Number of unique authors: 21891404\n",
      "\n",
      "Sample of encoded data:\n",
      "                                           hotel_url  hotel_id        author  \\\n",
      "0  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...         0  violettaf340   \n",
      "1  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...         0     Lagaiuzza   \n",
      "2  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...         0    ashleyn763   \n",
      "3  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...         0   DavideMauro   \n",
      "4  Hotel_Review-g194775-d1121769-Reviews-Hotel_Ba...         0      Alemma11   \n",
      "\n",
      "   author_id  \n",
      "0          0  \n",
      "1          1  \n",
      "2          2  \n",
      "3          3  \n",
      "4          4  \n"
     ]
    }
   ],
   "source": [
    "# Encode the data\n",
    "df_encoded, hotel_mapping, author_mapping = encode_hotel_author(df)\n",
    "\n",
    "# Now df_encoded contains new columns 'hotel_id' and 'author_id'\n",
    "# The original columns are preserved\n",
    "\n",
    "# Example usage:\n",
    "print(\"\\nSample of encoded data:\")\n",
    "print(df_encoded[['hotel_url', 'hotel_id', 'author', 'author_id']].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f0c5cb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metadata inference failed in `drop_by_shallow_copy`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nKeyError(\"['hotel_url', 'author'] not found in axis\")\n\nTraceback:\n---------\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py\", line 6876, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\utils.py\", line 774, in drop_by_shallow_copy\n    df2.drop(columns=columns, inplace=True, errors=errors)\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 5399, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\utils.py:193\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[1;34m(funcname, udf)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:6876\u001b[0m, in \u001b[0;36m_emulate\u001b[1;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6875\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(func), udf\u001b[38;5;241m=\u001b[39mudf), check_numeric_only_deprecation():\n\u001b[1;32m-> 6876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m_extract_meta(args, \u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_extract_meta(kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\utils.py:774\u001b[0m, in \u001b[0;36mdrop_by_shallow_copy\u001b[1;34m(df, columns, errors)\u001b[0m\n\u001b[0;32m    773\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [columns]\n\u001b[1;32m--> 774\u001b[0m df2\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mcolumns, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df2\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5263\u001b[0m \u001b[38;5;124;03mDrop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5264\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5397\u001b[0m \u001b[38;5;124;03m        weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 5399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5400\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5401\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5402\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5403\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5404\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5405\u001b[0m     inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5406\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5407\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4546\u001b[0m     new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4547\u001b[0m indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6934\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6935\u001b[0m indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['hotel_url', 'author'] not found in axis\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Remove hotel_url and author columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m df_encoded\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhotel_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# keeping rows with rating value, removing empty rating rows\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m df_encoded[df_encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:5599\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, columns, errors)\u001b[0m\n\u001b[0;32m   5595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_partitions(\n\u001b[0;32m   5596\u001b[0m         drop_by_shallow_copy, columns, errors\u001b[38;5;241m=\u001b[39merrors, enforce_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   5597\u001b[0m     )\n\u001b[0;32m   5598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 5599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_partitions(\n\u001b[0;32m   5600\u001b[0m         drop_by_shallow_copy, labels, errors\u001b[38;5;241m=\u001b[39merrors, enforce_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   5601\u001b[0m     )\n\u001b[0;32m   5602\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   5603\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDrop currently only works for axis=1 or when columns is not None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5604\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:1006\u001b[0m, in \u001b[0;36m_Frame.map_partitions\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;129m@insert_meta_param_description\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_partitions\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply Python function on each DataFrame partition.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m    Note that the index and divisions are assumed to remain unchanged.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;124;03m    None as the division.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m map_partitions(func, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:6946\u001b[0m, in \u001b[0;36mmap_partitions\u001b[1;34m(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6939\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   6940\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt want the partitions to be aligned, and are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `map_partitions` directly, pass `align_dataframes=False`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6942\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   6944\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [df \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, _Frame)]\n\u001b[1;32m-> 6946\u001b[0m meta \u001b[38;5;241m=\u001b[39m _get_meta_map_partitions(args, dfs, func, kwargs, meta, parent_meta)\n\u001b[0;32m   6947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Scalar) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[0;32m   6948\u001b[0m     layer \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   6949\u001b[0m         (name, \u001b[38;5;241m0\u001b[39m): (\n\u001b[0;32m   6950\u001b[0m             apply,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6954\u001b[0m         )\n\u001b[0;32m   6955\u001b[0m     }\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:7057\u001b[0m, in \u001b[0;36m_get_meta_map_partitions\u001b[1;34m(args, dfs, func, kwargs, meta, parent_meta)\u001b[0m\n\u001b[0;32m   7053\u001b[0m     parent_meta \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_meta\n\u001b[0;32m   7054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m no_default:\n\u001b[0;32m   7055\u001b[0m     \u001b[38;5;66;03m# Use non-normalized kwargs here, as we want the real values (not\u001b[39;00m\n\u001b[0;32m   7056\u001b[0m     \u001b[38;5;66;03m# delayed values)\u001b[39;00m\n\u001b[1;32m-> 7057\u001b[0m     meta \u001b[38;5;241m=\u001b[39m _emulate(func, \u001b[38;5;241m*\u001b[39margs, udf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   7058\u001b[0m     meta_is_emulated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   7059\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:6875\u001b[0m, in \u001b[0;36m_emulate\u001b[1;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_emulate\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, udf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   6871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6872\u001b[0m \u001b[38;5;124;03m    Apply a function using args / kwargs. If arguments contain dd.DataFrame /\u001b[39;00m\n\u001b[0;32m   6873\u001b[0m \u001b[38;5;124;03m    dd.Series, using internal cache (``_meta``) for calculation\u001b[39;00m\n\u001b[0;32m   6874\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6875\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(func), udf\u001b[38;5;241m=\u001b[39mudf), check_numeric_only_deprecation():\n\u001b[0;32m   6876\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m_extract_meta(args, \u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_extract_meta(kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    153\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\utils.py:214\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[1;34m(funcname, udf)\u001b[0m\n\u001b[0;32m    205\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error is below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m )\n\u001b[0;32m    213\u001b[0m msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m funcname \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(e), tb)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Metadata inference failed in `drop_by_shallow_copy`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nKeyError(\"['hotel_url', 'author'] not found in axis\")\n\nTraceback:\n---------\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py\", line 6876, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\utils.py\", line 774, in drop_by_shallow_copy\n    df2.drop(columns=columns, inplace=True, errors=errors)\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 5399, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\n"
     ]
    }
   ],
   "source": [
    "# Remove hotel_url and author columns\n",
    "df_encoded = df_encoded.drop(['hotel_url', 'author'], axis=1)\n",
    "# keeping rows with rating value, removing empty rating rows\n",
    "df_encoded = df_encoded[df_encoded['rating'].notnull()]\n",
    "\n",
    "# # Step 1: Get value counts for authors\n",
    "# print(\"Counting reviews per author...\")\n",
    "# author_counts = df_encoded['author_id'].value_counts().compute()\n",
    "\n",
    "# # Step 2: Get list of authors with more than 20 reviews\n",
    "# print(\"Getting list of frequent authors...\")\n",
    "# frequent_authors = list(author_counts[author_counts >= 20].index)\n",
    "# print(f\"Found {len(frequent_authors)} authors with 20+ reviews\")\n",
    "\n",
    "# # Step 3: Filter and save in chunks\n",
    "# print(\"Filtering and saving data...\")\n",
    "# df_frequent = df_encoded[df_encoded['author_id'].isin(frequent_authors)]\n",
    "\n",
    "# # Save to CSV in chunks\n",
    "# df_frequent.to_csv('../data/hotel_reviews_frequent_reviewers_*.csv', \n",
    "#                   single_file=True,  # Combines all parts into a single file\n",
    "#                   compute=True,      # Actually performs the computation\n",
    "#                   encoding='utf-8',\n",
    "#                   index=False)\n",
    "\n",
    "# print(\"Done! Checking file size...\")\n",
    "\n",
    "# # Check results\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "# # Get the saved file(s)\n",
    "# saved_file = glob.glob('../data/hotel_reviews_frequent_reviewers_*.csv')[0]\n",
    "# file_size = os.path.getsize(saved_file) / (1024 * 1024)  # Convert to MB\n",
    "# print(f\"File saved as: {saved_file}\")\n",
    "# print(f\"File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b69a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing npartitions...\n",
      "Computing author frequencies...\n",
      "[                                        ] | 0% Completed | 17.93 sms"
     ]
    }
   ],
   "source": [
    "# optimized version to keep only authors with 20+ reviews\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Enable progress bar\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "# Optimize partitions based on memory\n",
    "print(\"Computing npartitions...\")\n",
    "npartitions = 25\n",
    "df_encoded = df_encoded.repartition(npartitions=npartitions)\n",
    "\n",
    "# Compute author counts using optimized aggregation\n",
    "print(\"Computing author frequencies...\")\n",
    "author_counts = df_encoded.author_id.value_counts(split_every=8).compute()\n",
    "print(\"Computing frequent authors...\")\n",
    "frequent_authors = set(author_counts[author_counts >= 20].index)\n",
    "print(f\"Found {len(frequent_authors)} frequent authors\")\n",
    "\n",
    "# Filter with optimized settings\n",
    "print(\"Filtering and saving data...\")\n",
    "df_frequent = df_encoded[df_encoded.author_id.isin(frequent_authors)]\n",
    "\n",
    "# Save with optimized settings\n",
    "df_frequent.to_csv('../data/hotel_reviews_frequent_reviewers_*.csv',\n",
    "                   single_file=True,\n",
    "                   compute=True,\n",
    "                   encoding='utf-8',\n",
    "                   index=False,\n",
    "                   compression='gzip',\n",
    "                   blocksize='64MB')\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Count the number of reviews per author\n",
    "print(\"counting authors\")\n",
    "author_review_counts = df_encoded.groupby('author_id').size()\n",
    "print(\"counting authors done\")\n",
    "\n",
    "# Step 2: Filter for authors who have more than 20 reviews\n",
    "print(\"filtering authors\")\n",
    "authors_with_20_plus_reviews = author_review_counts[author_review_counts > 20].index\n",
    "print(\"filtering authors done\")\n",
    "\n",
    "print(\"filtering df\")\n",
    "# Step 3: Filter the original DataFrame for only these authors\n",
    "filtered_df = df[df['author_id'].isin(authors_with_20_plus_reviews)]\n",
    "print(\"filtering df done\")\n",
    "\n",
    "# Now filtered_df contains only entries where the author has more than 20 reviews\n",
    "# Export the filtered DataFrame to a CSV file\n",
    "filtered_df.to_csv('filtered_reviews.csv', index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
