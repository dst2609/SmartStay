{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b830ee55-99c8-4c5e-ab4d-3b7526c294be",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2761538-2617-4abd-a009-92af17c3a917",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b53f13a-eb94-4dc1-b3dd-dcaab3a975a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96db33-bdfc-43c9-855b-d2aaaa75000d",
   "metadata": {},
   "source": [
    "## Pre process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b1dc84-90a7-4e8b-b65e-240f980251bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 189992, Number of items: 329340\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Map author_id (user) and hotel_id (item) to continuous indices\n",
    "    user_mapping = {id: idx for idx, id in enumerate(df['author_id'].unique())}\n",
    "    item_mapping = {id: idx for idx, id in enumerate(df['hotel_id'].unique())}\n",
    "\n",
    "    df['author_id'] = df['author_id'].map(user_mapping)\n",
    "    df['hotel_id'] = df['hotel_id'].map(item_mapping)\n",
    "\n",
    "    num_users = len(user_mapping)\n",
    "    num_items = len(item_mapping)\n",
    "\n",
    "    return df, num_users, num_items\n",
    "\n",
    "# Load the data\n",
    "data_file = \"../data/combined_filtered_reviews.csv\"\n",
    "df, num_users, num_items = load_data(data_file)\n",
    "print(f\"Number of users: {num_users}, Number of items: {num_items}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a5fe3b-712a-4d9a-9c71-0f61b1523c3a",
   "metadata": {},
   "source": [
    "## NCFDataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc35982-baa1-418b-82f4-4cc254fc40d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset and DataLoader\n",
    "class NCFDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['author_id'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['hotel_id'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "# Split data into train and test sets\n",
    "dataset = NCFDataset(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "print(\"Data split into train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a38429-836b-45e1-a313-bbba9b7cdac1",
   "metadata": {},
   "source": [
    "## NCF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d3090e-bc1c-4821-81eb-1a0e5e411f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF(\n",
      "  (user_embedding): Embedding(189992, 50)\n",
      "  (item_embedding): Embedding(329340, 50)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the NCF model\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim=50):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, latent_dim)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_emb = self.user_embedding(user)\n",
    "        item_emb = self.item_embedding(item)\n",
    "        x = torch.cat([user_emb, item_emb], dim=-1)\n",
    "        return self.fc(x).squeeze()\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 50\n",
    "model = NCF(num_users, num_items, latent_dim).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f28f53-d50a-4276-8368-c00a032442ca",
   "metadata": {},
   "source": [
    "## Training setup loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d21e0e4-28ac-4dca-8df5-5393b35e21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e78ae10d-93ac-4a02-8545-e933df865f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 23062/23062 [02:05<00:00, 184.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.0264, Validation Loss: 0.8603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 23062/23062 [02:04<00:00, 184.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 0.8158, Validation Loss: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 23062/23062 [02:05<00:00, 184.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 0.7618, Validation Loss: 0.7830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 23062/23062 [02:05<00:00, 184.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 0.7385, Validation Loss: 0.7757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 23062/23062 [02:05<00:00, 183.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 0.7241, Validation Loss: 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 23062/23062 [02:05<00:00, 183.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 0.7117, Validation Loss: 0.7739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 23062/23062 [02:04<00:00, 184.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Train Loss: 0.6983, Validation Loss: 0.7782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 23062/23062 [02:05<00:00, 183.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Train Loss: 0.6840, Validation Loss: 0.7857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 23062/23062 [02:05<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Train Loss: 0.6684, Validation Loss: 0.7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 23062/23062 [02:05<00:00, 184.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 0.6523, Validation Loss: 0.7912\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{epochs}\") as pbar:\n",
    "        for user, item, rating in train_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(user, item)\n",
    "            loss = criterion(preds, rating)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for user, item, rating in test_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "            preds = model(user, item)\n",
    "            val_loss += criterion(preds, rating).item()\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86b096e7-2e42-432e-ac4d-813888756e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'ncf_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"ncf_model.pth\")\n",
    "print(\"Model saved as 'ncf_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852afa3-800b-4e6f-9f16-5d05384f73f9",
   "metadata": {},
   "source": [
    "## MSE and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b119a6c-eb04-4b99-8940-1943d51d1791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (MSE): 0.7912\n"
     ]
    }
   ],
   "source": [
    "# Reload the trained model\n",
    "state_dict = torch.load(\"ncf_model.pth\", weights_only=True)\n",
    "loaded_model = NCF(num_users, num_items, latent_dim=50)  # model architecture\n",
    "loaded_model.load_state_dict(state_dict)\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()  # Set to evaluation mode\n",
    "\n",
    "def compute_rmse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user, item, rating in data_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "            predictions = model(user, item)\n",
    "            mse_loss += torch.sum((predictions - rating) ** 2).item()\n",
    "            total_samples += len(rating)\n",
    "\n",
    "    rmse = math.sqrt(mse_loss / total_samples)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for user, item, rating in data_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "            predictions = model(user, item)\n",
    "            loss = criterion(predictions, rating)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "test_loss = evaluate_model(loaded_model, test_loader)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "\n",
    "test_rmse = compute_rmse(loaded_model, test_loader)\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48013ec9-943b-4bb4-a80e-ed571db7a20e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
